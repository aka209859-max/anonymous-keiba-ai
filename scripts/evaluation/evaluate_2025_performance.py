#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 7-8ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ—§ãƒ¢ãƒ‡ãƒ«ï¼ˆPhase 3-4-5ï¼‰ã¨æ–°ãƒ¢ãƒ‡ãƒ«ï¼ˆPhase 7-8-5ï¼‰ã®ç²¾åº¦ã‚’æ¯”è¼ƒ

ä½¿ç”¨ä¾‹:
    python scripts/evaluation/evaluate_2025_performance.py \
        --old-predictions data/predictions/old_model/funabashi_2025_predictions.csv \
        --new-predictions data/predictions/new_model/funabashi_2025_predictions.csv \
        --actuals data/actuals/funabashi_2025_actuals.csv \
        --output-report data/evaluation/funabashi_comparison_report.json
"""

import argparse
import pandas as pd
import numpy as np
import json
from pathlib import Path
from sklearn.metrics import ndcg_score
from scipy.stats import spearmanr


def load_data(predictions_csv, actuals_csv):
    """
    äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã¨å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    
    Parameters:
    -----------
    predictions_csv : str
        äºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    actuals_csv : str
        å®Ÿç¸¾çµæœãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    
    Returns:
    --------
    merged : pd.DataFrame
        ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    """
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
    encodings = ['shift-jis', 'utf-8', 'cp932']
    
    for enc in encodings:
        try:
            preds = pd.read_csv(predictions_csv, encoding=enc)
            print(f"âœ… äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {predictions_csv} ({enc})")
            break
        except UnicodeDecodeError:
            continue
    else:
        raise ValueError(f"âŒ äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {predictions_csv}")
    
    for enc in encodings:
        try:
            actuals = pd.read_csv(actuals_csv, encoding=enc)
            print(f"âœ… å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {actuals_csv} ({enc})")
            break
        except UnicodeDecodeError:
            continue
    else:
        raise ValueError(f"âŒ å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {actuals_csv}")
    
    # ãƒãƒ¼ã‚¸
    merged = pd.merge(preds, actuals, on=['race_id', 'umaban'], how='inner', suffixes=('_pred', '_actual'))
    
    print(f"ğŸ“Š ãƒãƒ¼ã‚¸çµæœ: {len(merged)} ãƒ¬ã‚³ãƒ¼ãƒ‰, {merged['race_id'].nunique()} ãƒ¬ãƒ¼ã‚¹")
    
    return merged


def calculate_hit_rates(merged):
    """
    çš„ä¸­ç‡ã‚’è¨ˆç®—
    
    Parameters:
    -----------
    merged : pd.DataFrame
        ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼ˆfinal_rank, actual_rank ã‚’å«ã‚€ï¼‰
    
    Returns:
    --------
    hit_rates : dict
        å„ç¨®çš„ä¸­ç‡
    """
    total_races = merged['race_id'].nunique()
    
    # å˜å‹çš„ä¸­ç‡: äºˆæ¸¬1ä½ãŒå®Ÿéš›ã«1ç€
    win_hits = 0
    for race_id in merged['race_id'].unique():
        race = merged[merged['race_id'] == race_id]
        predicted_1st = race[race['final_rank'] == 1]['umaban'].values
        actual_1st = race[race['actual_rank'] == 1]['umaban'].values
        
        if len(predicted_1st) > 0 and len(actual_1st) > 0:
            if predicted_1st[0] == actual_1st[0]:
                win_hits += 1
    
    hit_rate_win = win_hits / total_races if total_races > 0 else 0
    
    # è¤‡å‹çš„ä¸­ç‡: äºˆæ¸¬1ä½ãŒå®Ÿéš›ã«1ã€œ3ç€
    place_hits = 0
    for race_id in merged['race_id'].unique():
        race = merged[merged['race_id'] == race_id]
        predicted_1st = race[race['final_rank'] == 1]['umaban'].values
        actual_top3 = race[race['actual_rank'] <= 3]['umaban'].values
        
        if len(predicted_1st) > 0:
            if predicted_1st[0] in actual_top3:
                place_hits += 1
    
    hit_rate_place = place_hits / total_races if total_races > 0 else 0
    
    # é¦¬é€£çš„ä¸­ç‡: äºˆæ¸¬1-2ä½ãŒå®Ÿéš›ã®1-2ç€ã‚’å«ã‚€
    quinella_hits = 0
    for race_id in merged['race_id'].unique():
        race = merged[merged['race_id'] == race_id]
        predicted_top2 = set(race[race['final_rank'] <= 2]['umaban'].values)
        actual_top2 = set(race[race['actual_rank'] <= 2]['umaban'].values)
        
        if len(predicted_top2) == 2 and len(actual_top2) == 2:
            if predicted_top2 == actual_top2:
                quinella_hits += 1
    
    hit_rate_quinella = quinella_hits / total_races if total_races > 0 else 0
    
    # 3é€£è¤‡çš„ä¸­ç‡: äºˆæ¸¬1-3ä½ãŒå®Ÿéš›ã®1-3ç€ã‚’å«ã‚€
    trio_hits = 0
    for race_id in merged['race_id'].unique():
        race = merged[merged['race_id'] == race_id]
        predicted_top3 = set(race[race['final_rank'] <= 3]['umaban'].values)
        actual_top3 = set(race[race['actual_rank'] <= 3]['umaban'].values)
        
        if len(predicted_top3) == 3 and len(actual_top3) == 3:
            if predicted_top3 == actual_top3:
                trio_hits += 1
    
    hit_rate_trio = trio_hits / total_races if total_races > 0 else 0
    
    return {
        'hit_rate_win': hit_rate_win,
        'hit_rate_place': hit_rate_place,
        'hit_rate_quinella': hit_rate_quinella,
        'hit_rate_trio': hit_rate_trio,
        'total_races': total_races
    }


def calculate_ranking_metrics(merged):
    """
    äºˆæ¸¬ç²¾åº¦æŒ‡æ¨™ã‚’è¨ˆç®—
    
    Parameters:
    -----------
    merged : pd.DataFrame
        ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    
    Returns:
    --------
    metrics : dict
        NDCG@3, MAE, Spearmanç›¸é–¢
    """
    ndcg_scores = []
    mae_list = []
    spearman_list = []
    
    for race_id in merged['race_id'].unique():
        race = merged[merged['race_id'] == race_id].copy()
        
        # NDCG@3 è¨ˆç®—
        # çœŸã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°: ç€é †ãŒå°ã•ã„ã»ã©è‰¯ã„ â†’ relevance scoreã«å¤‰æ›
        y_true = []
        for rank in race['actual_rank']:
            if rank <= 3:
                # 1ç€=3ç‚¹, 2ç€=2ç‚¹, 3ç€=1ç‚¹
                y_true.append(4 - rank)
            else:
                y_true.append(0)
        
        y_true = [y_true]  # ndcg_scoreã¯2Dé…åˆ—ã‚’è¦æ±‚
        
        # äºˆæ¸¬ã‚¹ã‚³ã‚¢: ensemble_scoreãŒé«˜ã„ã»ã©è‰¯ã„
        if 'ensemble_score' in race.columns:
            y_pred = [race['ensemble_score'].tolist()]
        else:
            # ensemble_scoreãŒãªã„å ´åˆã¯final_rankã‹ã‚‰é€†ç®—
            y_pred = [[1.0 / (r + 1) for r in race['final_rank']]]
        
        try:
            ndcg = ndcg_score(y_true, y_pred, k=3)
            ndcg_scores.append(ndcg)
        except:
            pass
        
        # MAEï¼ˆå¹³å‡ç€é †èª¤å·®ï¼‰
        mae = np.mean(np.abs(race['final_rank'] - race['actual_rank']))
        mae_list.append(mae)
        
        # ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢
        try:
            corr, _ = spearmanr(race['final_rank'], race['actual_rank'])
            if not np.isnan(corr):
                spearman_list.append(corr)
        except:
            pass
    
    return {
        'ndcg_3': np.mean(ndcg_scores) if ndcg_scores else 0,
        'mae': np.mean(mae_list) if mae_list else 0,
        'spearman': np.mean(spearman_list) if spearman_list else 0
    }


def evaluate_model(predictions_csv, actuals_csv):
    """
    ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦ã‚’è©•ä¾¡
    
    Parameters:
    -----------
    predictions_csv : str
        äºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«
    actuals_csv : str
        å®Ÿç¸¾çµæœãƒ•ã‚¡ã‚¤ãƒ«
    
    Returns:
    --------
    metrics : dict
        è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
    """
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    merged = load_data(predictions_csv, actuals_csv)
    
    # çš„ä¸­ç‡
    hit_rates = calculate_hit_rates(merged)
    
    # äºˆæ¸¬ç²¾åº¦
    ranking_metrics = calculate_ranking_metrics(merged)
    
    # çµ±åˆ
    metrics = {**hit_rates, **ranking_metrics}
    
    return metrics


def compare_models(old_predictions, new_predictions, actuals, output_report):
    """
    æ—§ãƒ¢ãƒ‡ãƒ«ã¨æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ
    
    Parameters:
    -----------
    old_predictions : str
        æ—§ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœ
    new_predictions : str
        æ–°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœ
    actuals : str
        å®Ÿç¸¾çµæœ
    output_report : str
        å‡ºåŠ›ãƒ¬ãƒãƒ¼ãƒˆãƒ‘ã‚¹
    """
    print("=" * 60)
    print("ğŸ” æ—§ãƒ¢ãƒ‡ãƒ«ï¼ˆPhase 3-4-5ï¼‰ã®è©•ä¾¡")
    print("=" * 60)
    old_metrics = evaluate_model(old_predictions, actuals)
    
    print("\n" + "=" * 60)
    print("ğŸš€ æ–°ãƒ¢ãƒ‡ãƒ«ï¼ˆPhase 7-8-5ï¼‰ã®è©•ä¾¡")
    print("=" * 60)
    new_metrics = evaluate_model(new_predictions, actuals)
    
    # æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ
    print("\n" + "=" * 60)
    print("ğŸ“Š æ—§ãƒ¢ãƒ‡ãƒ« vs æ–°ãƒ¢ãƒ‡ãƒ« æ¯”è¼ƒçµæœ")
    print("=" * 60)
    
    comparison = {}
    
    for key in old_metrics:
        if key == 'total_races':
            continue
        
        old_val = old_metrics[key]
        new_val = new_metrics[key]
        
        if old_val > 0:
            improvement = ((new_val - old_val) / old_val) * 100
        else:
            improvement = 0
        
        comparison[key] = {
            'old_model': old_val,
            'new_model': new_val,
            'improvement_pct': improvement
        }
        
        if key.startswith('hit_rate'):
            print(f"{key:20s}: {old_val:6.2%} â†’ {new_val:6.2%} ({improvement:+6.2f}%)")
        else:
            print(f"{key:20s}: {old_val:6.3f} â†’ {new_val:6.3f} ({improvement:+6.2f}%)")
    
    # JSONå‡ºåŠ›
    output_data = {
        'old_model': old_metrics,
        'new_model': new_metrics,
        'comparison': comparison
    }
    
    output_path = Path(output_report)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_report}")


def main():
    parser = argparse.ArgumentParser(description='Phase 7-8ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡')
    parser.add_argument('--old-predictions', required=True, help='æ—§ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœCSV')
    parser.add_argument('--new-predictions', required=True, help='æ–°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœCSV')
    parser.add_argument('--actuals', required=True, help='å®Ÿç¸¾çµæœCSV')
    parser.add_argument('--output-report', required=True, help='å‡ºåŠ›ãƒ¬ãƒãƒ¼ãƒˆJSON')
    
    args = parser.parse_args()
    
    compare_models(
        args.old_predictions,
        args.new_predictions,
        args.actuals,
        args.output_report
    )


if __name__ == '__main__':
    main()
