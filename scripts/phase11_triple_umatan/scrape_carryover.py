#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒˆãƒªãƒ—ãƒ«é¦¬å˜ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å—é–¢æ±4å ´ï¼ˆæµ¦å’Œã€èˆ¹æ©‹ã€å¤§äº•ã€å·å´ï¼‰ã€é–€åˆ¥ã€åœ’ç”°ã€å§«è·¯ã«å¯¾å¿œ

ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹:
- nankankeiba.comï¼ˆå—é–¢æ±4å ´ï¼‰
- spat4.jpï¼ˆé–€åˆ¥ã€åœ’ç”°ã€å§«è·¯ï¼‰
"""

import requests
from bs4 import BeautifulSoup
import re
import datetime
import json
import logging
from pathlib import Path
from typing import Dict, Optional

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class TripleUmatanCarryoverScraper:
    """ãƒˆãƒªãƒ—ãƒ«é¦¬å˜ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        self.nankan_url = "https://www.nankankeiba.com/"
        self.spat4_url = "https://www.spat4.jp/"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        # ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        self.venue_map = {
            'æµ¦å’Œ': 42,
            'èˆ¹æ©‹': 43,
            'å¤§äº•': 44,
            'å·å´': 45,
            'é–€åˆ¥': 30,
            'åœ’ç”°': 50,
            'å§«è·¯': 51
        }
        
        # ãƒ•ãƒ«ã‚²ãƒ¼ãƒˆé ­æ•°ãƒãƒƒãƒ”ãƒ³ã‚°
        self.fullgate_map = {
            42: 14,  # æµ¦å’Œ
            43: 14,  # èˆ¹æ©‹
            44: 16,  # å¤§äº•
            45: 14,  # å·å´
            30: 16,  # é–€åˆ¥
            50: 14,  # åœ’ç”°ï¼ˆæ¨å®šï¼‰
            51: 14   # å§«è·¯ï¼ˆæ¨å®šï¼‰
        }
    
    def parse_japanese_amount(self, text: str) -> int:
        """
        æ—¥æœ¬èªã®é‡‘é¡è¡¨è¨˜ï¼ˆä¾‹ï¼š2å„„3000ä¸‡å††ï¼‰ã‚’æ•´æ•°ã«å¤‰æ›ã™ã‚‹
        
        Args:
            text: é‡‘é¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¾‹ï¼š11å„„1321ä¸‡0000å††ï¼‰
        
        Returns:
            int: é‡‘é¡ï¼ˆå††ï¼‰
        """
        if not text:
            return 0
        
        # ä¸è¦ãªç©ºç™½ã‚’é™¤å»
        text = text.strip()
        
        # 0å††åˆ¤å®š
        if "ãªã—" in text or "ã‚ã‚Šã¾ã›ã‚“" in text or text == "-" or "0å††" in text:
            return 0
        
        # æ•°å€¤è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯
        total = 0
        
        # 'å„„' ã®å‡¦ç†
        oku_match = re.search(r'(\d+)å„„', text)
        if oku_match:
            total += int(oku_match.group(1)) * 100_000_000
        
        # 'ä¸‡' ã®å‡¦ç†
        man_match = re.search(r'(\d+)ä¸‡', text)
        if man_match:
            total += int(man_match.group(1)) * 10_000
        
        # 'å††' ã®å‰ã®ç«¯æ•°å‡¦ç†
        # å„„ãƒ»ä¸‡ãŒå«ã¾ã‚Œãªã„ç´”ç²‹ãªæ•°å­—ã®ã¿ã®ã‚±ãƒ¼ã‚¹
        if total == 0:
            simple_digit = re.sub(r'[^\d]', '', text)
            if simple_digit:
                total = int(simple_digit)
        
        return total
    
    def fetch_nankan_carryover(self) -> Dict[str, int]:
        """
        nankankeiba.com ã‹ã‚‰å—é–¢æ±4å ´ã®ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±ã‚’å–å¾—
        
        Returns:
            dict: {'æµ¦å’Œ': é‡‘é¡, 'èˆ¹æ©‹': é‡‘é¡, 'å¤§äº•': é‡‘é¡, 'å·å´': é‡‘é¡}
        """
        result = {}
        
        try:
            response = requests.get(self.nankan_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # SPAT4 LOTO ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç‰¹å®š
            logo = soup.find('img', alt=re.compile(r'SPAT4.*LOTO', re.IGNORECASE))
            if not logo:
                logging.warning("SPAT4 LOTO logo not found on nankankeiba.com")
                return result
            
            # ãƒ­ã‚´ã‚’å«ã‚€ã‚³ãƒ³ãƒ†ãƒŠã¸ç§»å‹•
            container = logo.find_parent('div')
            if not container:
                logging.warning("SPAT4 LOTO container not found")
                return result
            
            # ã‚³ãƒ³ãƒ†ãƒŠå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æ
            text = container.get_text()
            
            # å„ç«¶é¦¬å ´ã®ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±ã‚’æŠ½å‡º
            for venue_name in ['æµ¦å’Œ', 'èˆ¹æ©‹', 'å¤§äº•', 'å·å´']:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³: ç«¶é¦¬å ´å + ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼ + é‡‘é¡
                pattern = rf'{venue_name}.*?ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼[ï¼š:]\s*([^\n]+)'
                match = re.search(pattern, text, re.DOTALL)
                
                if match:
                    amount_text = match.group(1).strip()
                    amount = self.parse_japanese_amount(amount_text)
                    result[venue_name] = amount
                    logging.info(f"âœ… {venue_name}: {amount:,}å†† ({amount_text})")
                else:
                    result[venue_name] = 0
                    logging.warning(f"âš ï¸ {venue_name}: ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        except Exception as e:
            logging.error(f"âŒ nankankeiba.com ã‹ã‚‰ã®å–å¾—ã«å¤±æ•—: {e}")
        
        return result
    
    def fetch_spat4_carryover(self) -> Dict[str, int]:
        """
        spat4.jp ã‹ã‚‰é–€åˆ¥ã€åœ’ç”°ã€å§«è·¯ã®ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±ã‚’å–å¾—
        
        Returns:
            dict: {'é–€åˆ¥': é‡‘é¡, 'åœ’ç”°': é‡‘é¡, 'å§«è·¯': é‡‘é¡}
        """
        result = {}
        
        try:
            response = requests.get(self.spat4_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # SPAT4 LOTO ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç‰¹å®šï¼ˆspat4.jpã®DOMæ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            # å®Ÿéš›ã®DOMæ§‹é€ ã«å¿œã˜ã¦è¦ä¿®æ­£
            text = soup.get_text()
            
            # å„ç«¶é¦¬å ´ã®ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±ã‚’æŠ½å‡º
            for venue_name in ['é–€åˆ¥', 'åœ’ç”°', 'å§«è·¯']:
                pattern = rf'{venue_name}.*?ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼[ï¼š:]\s*([^\n]+)'
                match = re.search(pattern, text, re.DOTALL)
                
                if match:
                    amount_text = match.group(1).strip()
                    amount = self.parse_japanese_amount(amount_text)
                    result[venue_name] = amount
                    logging.info(f"âœ… {venue_name}: {amount:,}å†† ({amount_text})")
                else:
                    result[venue_name] = 0
                    logging.info(f"â„¹ï¸ {venue_name}: ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±ãªã—")
        
        except Exception as e:
            logging.error(f"âŒ spat4.jp ã‹ã‚‰ã®å–å¾—ã«å¤±æ•—: {e}")
        
        return result
    
    def fetch_all_carryover(self) -> Dict[int, Dict[str, any]]:
        """
        å…¨ç«¶é¦¬å ´ã®ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±ã‚’å–å¾—
        
        Returns:
            dict: {ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰: {'venue_name': 'æµ¦å’Œ', 'carryover': é‡‘é¡, 'fullgate': 14}}
        """
        all_data = {}
        
        # å—é–¢æ±4å ´
        nankan_data = self.fetch_nankan_carryover()
        
        # é–€åˆ¥ã€åœ’ç”°ã€å§«è·¯
        spat4_data = self.fetch_spat4_carryover()
        
        # çµ±åˆ
        combined_data = {**nankan_data, **spat4_data}
        
        for venue_name, amount in combined_data.items():
            venue_code = self.venue_map.get(venue_name)
            if venue_code:
                all_data[venue_code] = {
                    'venue_name': venue_name,
                    'venue_code': venue_code,
                    'carryover': amount,
                    'fullgate': self.fullgate_map.get(venue_code, 14),
                    'timestamp': datetime.datetime.now().isoformat()
                }
        
        return all_data
    
    def save_to_json(self, data: Dict, output_path: str):
        """
        JSONå½¢å¼ã§ä¿å­˜
        
        Args:
            data: ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
            output_path: å‡ºåŠ›å…ˆãƒ‘ã‚¹
        """
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ğŸ’¾ ä¿å­˜å®Œäº†: {output_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    scraper = TripleUmatanCarryoverScraper()
    
    print("="*80)
    print("ãƒˆãƒªãƒ—ãƒ«é¦¬å˜ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±å–å¾—")
    print("="*80)
    
    # å…¨ç«¶é¦¬å ´ã®ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±ã‚’å–å¾—
    carryover_data = scraper.fetch_all_carryover()
    
    # çµæœè¡¨ç¤º
    print("\nğŸ“Š å–å¾—çµæœ:")
    print("-"*80)
    
    total_carryover = 0
    for venue_code, info in sorted(carryover_data.items()):
        venue_name = info['venue_name']
        carryover = info['carryover']
        fullgate = info['fullgate']
        
        total_carryover += carryover
        
        if carryover > 0:
            print(f"ğŸ‡ {venue_name}ï¼ˆ{venue_code}ï¼‰: {carryover:,}å†† "
                  f"| ãƒ•ãƒ«ã‚²ãƒ¼ãƒˆ: {fullgate}é ­")
        else:
            print(f"   {venue_name}ï¼ˆ{venue_code}ï¼‰: ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼ãªã— "
                  f"| ãƒ•ãƒ«ã‚²ãƒ¼ãƒˆ: {fullgate}é ­")
    
    print("-"*80)
    print(f"ğŸ’° åˆè¨ˆã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼: {total_carryover:,}å††")
    print("="*80)
    
    # JSONä¿å­˜
    today = datetime.datetime.now().strftime("%Y%m%d")
    output_path = f"data/triple_umatan/carryover/carryover_{today}.json"
    scraper.save_to_json(carryover_data, output_path)
    
    print(f"\nâœ… ã‚­ãƒ£ãƒªãƒ¼ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±å–å¾—å®Œäº†")


if __name__ == "__main__":
    main()
