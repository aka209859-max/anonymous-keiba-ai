# ğŸš¨ åœ°æ–¹ç«¶é¦¬AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  ç²¾åº¦å›å¾©ãƒ—ãƒ©ãƒ³
## Phase 7-8-5 æ–°ãƒ¢ãƒ‡ãƒ« è¤‡å‹çš„ä¸­ç‡ 89% â†’ 50% å•é¡Œ å®Œå…¨è§£æ±ºç­–

---

## ğŸ“‹ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### ğŸ”´ æ·±åˆ»ãªå•é¡Œ
- **æ—§ãƒ¢ãƒ‡ãƒ«**: 1ãƒ»2ä½é¦¬ã®è¤‡å‹çš„ä¸­ç‡ **89%**
- **æ–°ãƒ¢ãƒ‡ãƒ«**: 1ãƒ»2ä½é¦¬ã®è¤‡å‹çš„ä¸­ç‡ **50%æœªæº€** â† å®Ÿç”¨ä¸å¯èƒ½
- **å½±éŸ¿ç¯„å›²**: å…¨14ç«¶é¦¬å ´
- **çµŒæ¸ˆçš„æå¤±**: å›åç‡ãŒæç›Šåˆ†å²ç‚¹ã‚’å¤§å¹…ã«ä¸‹å›ã‚‹

### âœ… ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µãƒ¼ãƒã«ã‚ˆã‚‹æ ¹æœ¬åŸå› ç‰¹å®š

ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µãƒ¼ãƒã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®**3ã¤ã®ç‹¬ç«‹ã—ãŸæ ¹æœ¬åŸå› **ãŒç‰¹å®šã•ã‚Œã¾ã—ãŸ:

#### **åŸå› 1: ã‚¤ãƒ³ãƒ•ãƒ©å±¤ï¼ˆãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè¡ŒåŸºç›¤ï¼‰ã®éšœå®³** ğŸš¨
- **ç—‡çŠ¶**: `chcp 65001` + BOM ã«ã‚ˆã‚‹ã‚µã‚¤ãƒ¬ãƒ³ãƒˆå®Ÿè¡Œå¤±æ•—
- **ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **: 
  - UTF-8 BOM (0xEF 0xBB 0xBF) ãŒcmd.exeã«èª¤èªè­˜ã•ã‚Œã‚‹
  - ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«å†…ã§ `chcp 65001` ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ãŒãšã‚Œã‚‹
  - ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚³ãƒãƒ³ãƒ‰ãŒ**å®Ÿè¡Œã•ã‚ŒãŸã‚ˆã†ã«è¦‹ãˆã¦å®Ÿéš›ã¯ã‚¹ã‚­ãƒƒãƒ—**ã•ã‚Œã‚‹
  - çµæœ: å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚„ç©ºãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ãƒ»äºˆæ¸¬ã—ã¦ã—ã¾ã†
- **è¨¼æ‹ **: 
  - å®Ÿè¡Œãƒ­ã‚°ã§ã¯ exit_code=0 (æˆåŠŸ) ã ãŒã€DBã¯æ›´æ–°ã•ã‚Œã¦ã„ãªã„
  - æ—¥æœ¬èªã‚³ãƒ¡ãƒ³ãƒˆè¡ŒãŒã‚³ãƒãƒ³ãƒ‰ã¨ã—ã¦èª¤èªè­˜ã•ã‚Œã‚‹
  - ç’°å¢ƒå¤‰æ•°è¨­å®š (`PYTHONIOENCODING=utf-8`) ãŒå£Šã‚Œã¦ `'NCODING'` ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹

#### **åŸå› 2: ç‰¹å¾´é‡é¸æŠå±¤ï¼ˆBorutaï¼‰ã®ä¸å®‰å®šæ€§** ğŸ“‰
- **ç—‡çŠ¶**: é‡è¦ãªç‰¹å¾´é‡ãŒé™¤å¤–ã•ã‚Œã€ãƒã‚¤ã‚ºç‰¹å¾´é‡ãŒé¸æŠã•ã‚Œã‚‹
- **ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **:
  - Boruta ã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡é¸æŠ
  - åœ°æ–¹ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã¯**å¤šé‡å…±ç·šæ€§ãŒé«˜ã**ã€**ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„**
  - Shadowç‰¹å¾´é‡ã¨ã®æ¯”è¼ƒã§ä¸å®‰å®šãªé¸æŠãŒç™ºç”Ÿ
  - ç«¶é¦¬å ´ã”ã¨ã«é¸æŠã•ã‚Œã‚‹ç‰¹å¾´é‡æ•°ãŒãƒãƒ©ãƒãƒ©ï¼ˆ24ã€œ31å€‹ï¼‰
- **è¨¼æ‹ **:
  - èˆ¹æ©‹: Binary 31, Ranking 25, Regression 24 â† å°‘ãªã™ãã‚‹
  - æ—§ãƒ¢ãƒ‡ãƒ«: 40ã€œ50ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ã¦ã„ãŸ
  - Phase 7 ãƒ¬ãƒãƒ¼ãƒˆ: å¹³å‡39%ã®ç‰¹å¾´é‡ã‚’å‰Šé™¤

#### **åŸå› 3: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å±¤ï¼ˆOptunaï¼‰ã®éå­¦ç¿’** ğŸ¯
- **ç—‡çŠ¶**: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ã¯é«˜ç²¾åº¦ã ãŒã€æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã§å¤§å¹…ã«ç²¾åº¦ä½ä¸‹
- **ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **:
  - Optunaã®ç›®çš„é–¢æ•°ãŒ **LogLossåé‡** â†’ æ¥µç«¯ã«ä¿å®ˆçš„ãªäºˆæ¸¬
  - æ™‚ç³»åˆ—åˆ†å‰²ãªã— â†’ æœªæ¥ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ãŒå­¦ç¿’ã«æ··å…¥
  - Recallï¼ˆå†ç¾ç‡ï¼‰ãŒæ¥µç«¯ã«ä½ã„ (â‰ˆ 0.012) â†’ ã»ã¨ã‚“ã©å½“ãŸã‚‰ãªã„
  - å›åç‡ã‚’ç„¡è¦– â†’ çš„ä¸­ã—ã¦ã‚‚å„²ã‹ã‚‰ãªã„äºˆæ¸¬ã«ãªã‚‹
- **è¨¼æ‹ **:
  - Recall â‰ˆ 0.012 (æœ¬æ¥ã¯ 0.3ã€œ0.5 å¿…è¦)
  - Return Rate < 1.0 (è³­ã‘ã‚‹ã¨æã™ã‚‹)
  - 1ä½äºˆæ¸¬ãŒå…¨ãƒ¬ãƒ¼ã‚¹ã§åŒã˜é¦¬ï¼ˆæ¥µç«¯ãªåã‚Šï¼‰

---

## ğŸ¯ è§£æ±ºç­– 3éšå±¤ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

### éšå±¤1: ã‚¤ãƒ³ãƒ•ãƒ©å±¤ã®å®Œå…¨ä¿®å¾© ğŸ”§

#### 1.1 ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ ¹æœ¬çš„å†è¨­è¨ˆ

**âŒ å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå¤±æ•—ï¼‰**
```batch
@echo off
chcp 65001 > nul
REM æ—¥æœ¬èªã‚³ãƒ¡ãƒ³ãƒˆ â† BOMã§å£Šã‚Œã‚‹
set PYTHONIOENCODING=utf-8 â† 'NCODING' ã‚¨ãƒ©ãƒ¼
python script.py â† å®Ÿè¡Œã•ã‚Œãªã„
```

**âœ… æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆè‡ªå·±å†å…¥æ§‹é€ ï¼‰**
```batch
@echo off
REM == Bootstrap Section (ASCII only, no Japanese) ==
chcp 65001 > nul
if "%~1"=="__REENTRY__" goto :MAIN_LOGIC

REM Relaunch self in new cmd.exe process
cmd /c "%~f0" __REENTRY__ %*
exit /b

:MAIN_LOGIC
shift /1
REM == Main Logic (now running in UTF-8 mode) ==
setlocal enabledelayedexpansion
set PYTHONUTF8=1
set PYTHONIOENCODING=utf-8

REM Japanese comments work here
echo å®Ÿè¡Œé–‹å§‹

REM Commands execute properly
python scripts\phase0_data_acquisition\extract_race_data.py %*
```

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ:**
1. **æœ€åˆã®3è¡Œã¯ ASCII ã®ã¿** â†’ BOM ã®å½±éŸ¿ã‚’å—ã‘ãªã„
2. **å³åº§ã«æ–°ãƒ—ãƒ­ã‚»ã‚¹ã§å†èµ·å‹•** â†’ UTF-8 ç’°å¢ƒã§å…¨ä½“ãŒè§£æã•ã‚Œã‚‹
3. **`__REENTRY__` ãƒ•ãƒ©ã‚°** â†’ ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²æ­¢
4. **æ—¥æœ¬èªã‚³ãƒ¡ãƒ³ãƒˆãƒ»å¤‰æ•°ã¯ `:MAIN_LOGIC` ä»¥é™ã®ã¿** â†’ å®‰å…¨

#### 1.2 ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ«ãƒ¼ãƒ«ã®å³æ ¼åŒ–

**VS Code è¨­å®š:**
```json
{
  "files.encoding": "utf8",
  "files.autoGuessEncoding": false,
  "files.eol": "\r\n",
  "[bat]": {
    "files.encoding": "utf8",
    "files.bomEncoding": "utf8"
  }
}
```

**ä¿å­˜æ™‚ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ:**
- [ ] ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: **UTF-8** (BOMç„¡ã—)
- [ ] æ”¹è¡Œã‚³ãƒ¼ãƒ‰: **CRLF** (Windowsæ¨™æº–)
- [ ] æœ€åˆã®3è¡Œ: **ASCIIæ–‡å­—ã®ã¿**
- [ ] æ—¥æœ¬èª: **`:MAIN_LOGIC` ä»¥é™ã®ã¿**

#### 1.3 PowerShell ã¸ã®æ®µéšçš„ç§»è¡Œï¼ˆé•·æœŸå¯¾ç­–ï¼‰

**ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã®å•é¡Œ:**
- cmd.exe ã¯1980å¹´ä»£ã®è¨­è¨ˆ
- UTF-8 ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆãªã—
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒä¸å®Œå…¨

**PowerShell ã®åˆ©ç‚¹:**
- UTF-8 ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆ
- æ§‹é€ åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- å¼·åŠ›ãªåˆ¶å¾¡æ§‹æ–‡
- ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  (PowerShell Core)

**ç§»è¡Œè¨ˆç”»:**
```powershell
# run_all_optimized.ps1
param(
    [Parameter(Mandatory=$true)]
    [int]$VenueCode,
    
    [Parameter(Mandatory=$true)]
    [string]$TargetDate
)

# UTF-8 è¨­å®š
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8
$env:PYTHONUTF8 = "1"
$env:PYTHONIOENCODING = "utf-8"

# ç«¶é¦¬å ´åãƒãƒƒãƒ”ãƒ³ã‚°
$VenueNames = @{
    30 = "é–€åˆ¥"; 35 = "ç››å²¡"; 36 = "æ°´æ²¢"
    42 = "æµ¦å’Œ"; 43 = "èˆ¹æ©‹"; 44 = "å¤§äº•"
    45 = "å·å´"; 46 = "é‡‘æ²¢"; 47 = "ç¬ æ¾"
    48 = "åå¤å±‹"; 50 = "åœ’ç”°"; 51 = "å§«è·¯"
    54 = "é«˜çŸ¥"; 55 = "ä½è³€"
}

$VenueName = $VenueNames[$VenueCode]
if (-not $VenueName) {
    Write-Error "Invalid venue code: $VenueCode"
    exit 1
}

Write-Host "===========================================" -ForegroundColor Cyan
Write-Host "  åœ°æ–¹ç«¶é¦¬AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ " -ForegroundColor Cyan
Write-Host "===========================================" -ForegroundColor Cyan
Write-Host "ç«¶é¦¬å ´: $VenueName (Code: $VenueCode)" -ForegroundColor Yellow
Write-Host "å®Ÿè¡Œæ—¥: $TargetDate" -ForegroundColor Yellow
Write-Host "===========================================" -ForegroundColor Cyan

# Phase 0: ãƒ‡ãƒ¼ã‚¿å–å¾—
Write-Host "`n[Phase 0] ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹..." -ForegroundColor Green
try {
    python scripts\phase0_data_acquisition\extract_race_data.py $VenueCode $TargetDate
    if ($LASTEXITCODE -ne 0) { throw "Phase 0 failed" }
    Write-Host "[Phase 0] å®Œäº† âœ“" -ForegroundColor Green
} catch {
    Write-Error "[Phase 0] å¤±æ•—: $_"
    exit 1
}

# Phase 1-8 åŒæ§˜ã«å®Ÿè£…...
```

---

### éšå±¤2: ç‰¹å¾´é‡é¸æŠã®æ”¹å–„ ğŸ“Š

#### 2.1 Boruta ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´

**ç¾åœ¨ã®è¨­å®šï¼ˆå•é¡Œã‚ã‚Šï¼‰:**
```python
# alpha=0.10 ã¯å³ã—ã™ãã‚‹ â†’ é‡è¦ãªç‰¹å¾´é‡ã‚‚é™¤å¤–ã•ã‚Œã‚‹
# max_iter=200 ã¯å¤šã™ãã‚‹ â†’ éå‰°ã«å®‰å®šæ€§ã‚’æ±‚ã‚ã¦å‰Šé™¤ã—ã™ãã‚‹
BorutaPy(
    estimator=rf,
    alpha=0.10,  # â† å³ã—ã™ã
    max_iter=200  # â† å¤šã™ã
)
```

**æ”¹å–„æ¡ˆ1: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç·©å’Œ**
```python
# alpha ã‚’ç·©ã‚ã¦ã€ã‚ˆã‚Šå¤šãã®ç‰¹å¾´é‡ã‚’æ®‹ã™
# max_iter ã‚’æ¸›ã‚‰ã—ã¦ã€æ—©æœŸã«æ±ºå®šã™ã‚‹
BorutaPy(
    estimator=rf,
    alpha=0.20,  # 0.10 â†’ 0.20 (ç·©å’Œ)
    max_iter=100,  # 200 â†’ 100 (å‰Šæ¸›)
    perc=80  # Shadowç‰¹å¾´é‡ã®ä¸‹ä½80%ã‚’é–¾å€¤ã«
)
```

**æ”¹å–„æ¡ˆ2: å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯ã®è¿½åŠ **
```python
def stable_boruta_selection(X, y, n_runs=5):
    """
    è¤‡æ•°å›Borutaã‚’å®Ÿè¡Œã—ã¦ã€å®‰å®šã—ã¦é¸æŠã•ã‚Œã‚‹ç‰¹å¾´é‡ã®ã¿ã‚’æ¡ç”¨
    """
    feature_counts = defaultdict(int)
    
    for run in range(n_runs):
        # ç•°ãªã‚‹ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
        rf = RandomForestClassifier(
            n_jobs=-1,
            class_weight='balanced',
            max_depth=7,
            random_state=42 + run  # ã‚·ãƒ¼ãƒ‰å¤‰æ›´
        )
        
        boruta = BorutaPy(
            estimator=rf,
            alpha=0.20,
            max_iter=100,
            random_state=42 + run
        )
        
        boruta.fit(X.values, y.values)
        
        # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for i, selected in enumerate(boruta.support_):
            if selected:
                feature_counts[X.columns[i]] += 1
    
    # 80%ä»¥ä¸Šã®ç¢ºç‡ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®ã¿æ¡ç”¨
    threshold = n_runs * 0.8
    stable_features = [
        feat for feat, count in feature_counts.items()
        if count >= threshold
    ]
    
    print(f"âœ… å®‰å®šé¸æŠç‰¹å¾´é‡: {len(stable_features)}å€‹")
    print(f"  - 5å›ä¸­4å›ä»¥ä¸Šé¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®ã¿æ¡ç”¨")
    
    return stable_features
```

#### 2.2 ä»£æ›¿æ‰‹æ³•ã®å°å…¥

**Option A: Lassoæ­£å‰‡åŒ–ï¼ˆL1æ­£å‰‡åŒ–ï¼‰**
```python
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler

def lasso_feature_selection(X, y, min_features=20):
    """
    Lassoæ­£å‰‡åŒ–ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ
    - å®‰å®šæ€§ãŒé«˜ã„
    - å¤šé‡å…±ç·šæ€§ã«å¼·ã„
    - è§£é‡ˆæ€§ãŒé«˜ã„
    """
    # æ¨™æº–åŒ–ï¼ˆLassoã¯å¿…é ˆï¼‰
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Cross-validation ã§æœ€é©ãªalpha ã‚’æ¢ç´¢
    lasso = LassoCV(
        alphas=np.logspace(-4, 1, 100),
        cv=5,
        max_iter=10000,
        random_state=42
    )
    lasso.fit(X_scaled, y)
    
    # ä¿‚æ•°ãŒ0ã§ãªã„ç‰¹å¾´é‡ã‚’é¸æŠ
    selected_mask = np.abs(lasso.coef_) > 1e-5
    selected_features = X.columns[selected_mask].tolist()
    
    # æœ€ä½é™ã®ç‰¹å¾´é‡æ•°ã‚’ç¢ºä¿
    if len(selected_features) < min_features:
        # ä¿‚æ•°ã®çµ¶å¯¾å€¤ãƒˆãƒƒãƒ— N ã‚’é¸æŠ
        top_indices = np.argsort(np.abs(lasso.coef_))[::-1][:min_features]
        selected_features = X.columns[top_indices].tolist()
    
    print(f"âœ… Lassoé¸æŠç‰¹å¾´é‡: {len(selected_features)}å€‹")
    print(f"  - æœ€é©æ­£å‰‡åŒ–ä¿‚æ•°: {lasso.alpha_:.6f}")
    
    return selected_features
```

**Option B: ç›¸äº’æƒ…å ±é‡ï¼ˆMutual Informationï¼‰**
```python
from sklearn.feature_selection import mutual_info_classif

def mutual_info_selection(X, y, top_k=30):
    """
    ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ
    - éç·šå½¢é–¢ä¿‚ã‚‚æ‰ãˆã‚‹
    - è¨ˆç®—ãŒé«˜é€Ÿ
    - ã—ãã„å€¤è¨­å®šãŒä¸è¦
    """
    # ç›¸äº’æƒ…å ±é‡ã‚’è¨ˆç®—
    mi_scores = mutual_info_classif(
        X, y,
        discrete_features='auto',
        random_state=42,
        n_neighbors=3
    )
    
    # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã« top_k å€‹ã‚’é¸æŠ
    top_indices = np.argsort(mi_scores)[::-1][:top_k]
    selected_features = X.columns[top_indices].tolist()
    
    # ã‚¹ã‚³ã‚¢è¡¨ç¤º
    print(f"âœ… ç›¸äº’æƒ…å ±é‡é¸æŠç‰¹å¾´é‡: {len(selected_features)}å€‹")
    print(f"\nTOP 10 ç‰¹å¾´é‡:")
    for i in top_indices[:10]:
        print(f"  - {X.columns[i]}: {mi_scores[i]:.4f}")
    
    return selected_features
```

**Option C: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ï¼ˆæ¨å¥¨ï¼‰**
```python
def hybrid_feature_selection(X, y):
    """
    è¤‡æ•°æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æœ€ã‚‚å®‰å®šã—ãŸç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’é¸æŠ
    """
    # 1. Boruta (å®‰å®šç‰ˆ)
    boruta_features = stable_boruta_selection(X, y, n_runs=5)
    
    # 2. Lasso
    lasso_features = lasso_feature_selection(X, y, min_features=20)
    
    # 3. ç›¸äº’æƒ…å ±é‡
    mi_features = mutual_info_selection(X, y, top_k=35)
    
    # 4. å¤šæ•°æ±º: 2ã¤ä»¥ä¸Šã®æ‰‹æ³•ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’æ¡ç”¨
    feature_votes = defaultdict(int)
    for feat in boruta_features:
        feature_votes[feat] += 1
    for feat in lasso_features:
        feature_votes[feat] += 1
    for feat in mi_features:
        feature_votes[feat] += 1
    
    # 2ã¤ä»¥ä¸Šã®æ‰‹æ³•ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡
    consensus_features = [
        feat for feat, count in feature_votes.items()
        if count >= 2
    ]
    
    print(f"\nâœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é¸æŠç‰¹å¾´é‡: {len(consensus_features)}å€‹")
    print(f"  - Boruta: {len(boruta_features)}å€‹")
    print(f"  - Lasso: {len(lasso_features)}å€‹")
    print(f"  - ç›¸äº’æƒ…å ±é‡: {len(mi_features)}å€‹")
    print(f"  - 2æ‰‹æ³•ä»¥ä¸Šã§ä¸€è‡´: {len(consensus_features)}å€‹")
    
    return consensus_features
```

#### 2.3 ç‰¹å¾´é‡æ•°ã®ä¸‹é™è¨­å®š

```python
# ç«¶é¦¬äºˆæ¸¬ã«å¿…è¦ãªæœ€ä½é™ã®ç‰¹å¾´é‡æ•°
MIN_FEATURES = {
    'binary': 30,      # å‡ºèµ°åˆ¤å®šã¯30ç‰¹å¾´é‡ä»¥ä¸Š
    'ranking': 35,     # é †ä½äºˆæ¸¬ã¯35ç‰¹å¾´é‡ä»¥ä¸Š
    'regression': 32   # ã‚¿ã‚¤ãƒ äºˆæ¸¬ã¯32ç‰¹å¾´é‡ä»¥ä¸Š
}

# é¸æŠå¾Œã«ãƒã‚§ãƒƒã‚¯
if len(selected_features) < MIN_FEATURES[model_type]:
    print(f"âš ï¸  è­¦å‘Š: ç‰¹å¾´é‡æ•°ãŒå°‘ãªã™ãã¾ã™ ({len(selected_features)} < {MIN_FEATURES[model_type]})")
    print(f"  - é‡è¦åº¦ä¸Šä½ {MIN_FEATURES[model_type]} å€‹ã‚’å¼·åˆ¶æ¡ç”¨ã—ã¾ã™")
    
    # é‡è¦åº¦è¨ˆç®—
    rf = RandomForestClassifier(n_jobs=-1, random_state=42)
    rf.fit(X, y)
    importances = rf.feature_importances_
    
    # é‡è¦åº¦ãƒˆãƒƒãƒ— N ã‚’é¸æŠ
    top_indices = np.argsort(importances)[::-1][:MIN_FEATURES[model_type]]
    selected_features = X.columns[top_indices].tolist()
```

---

### éšå±¤3: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã®å†è¨­è¨ˆ ğŸ¯

#### 3.1 Optuna ç›®çš„é–¢æ•°ã®æ”¹å–„

**âŒ ç¾åœ¨ã®ç›®çš„é–¢æ•°ï¼ˆå•é¡Œã‚ã‚Šï¼‰:**
```python
def objective(trial):
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'num_leaves': trial.suggest_int('num_leaves', 20, 200)
    }
    
    model = lgb.LGBMClassifier(**params)
    model.fit(X_train, y_train)
    
    y_pred = model.predict_proba(X_val)[:, 1]
    
    # LogLoss ã®ã¿ã‚’æœ€å°åŒ– â† ä¿å®ˆçš„ã™ãã‚‹
    return log_loss(y_val, y_pred)
```

**âœ… æ”¹å–„ç‰ˆç›®çš„é–¢æ•°ï¼ˆå¤šç›®çš„æœ€é©åŒ–ï¼‰:**
```python
def objective_multi_objective(trial):
    """
    è¤‡æ•°ã®æŒ‡æ¨™ã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãæœ€é©åŒ–
    - Recall: çš„ä¸­ç‡ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
    - LogLoss: ç¢ºç‡ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
    - ExpectedProfit: æœŸå¾…åˆ©ç›Šï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
    """
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'max_depth': trial.suggest_int('max_depth', 4, 8),
        'num_leaves': trial.suggest_int('num_leaves', 31, 100),
        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0)
    }
    
    model = lgb.LGBMClassifier(**params, random_state=42, n_jobs=-1)
    
    # æ™‚ç³»åˆ—åˆ†å‰²ã§è©•ä¾¡
    tscv = TimeSeriesSplit(n_splits=5)
    
    recalls = []
    logloss_scores = []
    profits = []
    
    for train_idx, val_idx in tscv.split(X):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
        
        model.fit(X_train_fold, y_train_fold)
        
        y_pred_proba = model.predict_proba(X_val_fold)[:, 1]
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # Recall è¨ˆç®—
        recall = recall_score(y_val_fold, y_pred)
        recalls.append(recall)
        
        # LogLoss è¨ˆç®—
        ll = log_loss(y_val_fold, y_pred_proba)
        logloss_scores.append(ll)
        
        # æœŸå¾…åˆ©ç›Šè¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        # ä»®å®š: è¤‡å‹ã‚ªãƒƒã‚ºå¹³å‡ 1.5å€ã€çš„ä¸­æ™‚åˆ©ç›Š 0.5å€
        hit_rate = recall
        expected_profit = hit_rate * 0.5 - (1 - hit_rate) * 1.0
        profits.append(expected_profit)
    
    # å„æŒ‡æ¨™ã®å¹³å‡
    avg_recall = np.mean(recalls)
    avg_logloss = np.mean(logloss_scores)
    avg_profit = np.mean(profits)
    
    # è¤‡åˆã‚¹ã‚³ã‚¢
    # - Recall ã‚’æœ€å¤§åŒ–ï¼ˆé‡ã¿: 0.5ï¼‰
    # - LogLoss ã‚’æœ€å°åŒ–ï¼ˆé‡ã¿: 0.3ï¼‰
    # - æœŸå¾…åˆ©ç›Šã‚’æœ€å¤§åŒ–ï¼ˆé‡ã¿: 0.2ï¼‰
    composite_score = (
        0.5 * avg_recall
        - 0.3 * avg_logloss  # ãƒã‚¤ãƒŠã‚¹ã§æœ€å°åŒ–
        + 0.2 * avg_profit
    )
    
    # Optuna ã¯æœ€å°åŒ–ã™ã‚‹ãŸã‚ã€ç¬¦å·ã‚’åè»¢
    return -composite_score
```

#### 3.2 æ™‚ç³»åˆ—åˆ†å‰²ã®å³æ ¼åŒ–

**å•é¡Œç‚¹:**
- ç¾åœ¨ã¯ **ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²** â†’ æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå­¦ç¿’ã«æ··å…¥
- ç«¶é¦¬ã¯æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ â†’ éå»ã®ãƒ‡ãƒ¼ã‚¿ã§æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹å¿…è¦

**æ”¹å–„ç­–:**
```python
from sklearn.model_selection import TimeSeriesSplit

# ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
df = df.sort_values(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])

# æ™‚ç³»åˆ—åˆ†å‰²
# - æœ€åˆã®80%ã§å­¦ç¿’
# - æœ€å¾Œã®20%ã§ãƒ†ã‚¹ãƒˆ
split_point = int(len(df) * 0.8)
train_df = df.iloc[:split_point]
test_df = df.iloc[split_point:]

print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {train_df['kaisai_tsukihi'].min()} ã€œ {train_df['kaisai_tsukihi'].max()}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_df['kaisai_tsukihi'].min()} ã€œ {test_df['kaisai_tsukihi'].max()}")

# Cross-validation ã‚‚æ™‚ç³»åˆ—åˆ†å‰²
tscv = TimeSeriesSplit(n_splits=5)
for train_idx, val_idx in tscv.split(train_df):
    # ...
```

#### 3.3 æ—©æœŸåœæ­¢ï¼ˆEarly Stoppingï¼‰ã®å°å…¥

```python
# å­¦ç¿’æ™‚ã«æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§æ—©æœŸåœæ­¢
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,  # 50å›æ”¹å–„ãŒãªã‘ã‚Œã°åœæ­¢
    verbose=100
)

print(f"âœ… æœ€é©ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {model.best_iteration_}")
print(f"âœ… æ¤œè¨¼ã‚¹ã‚³ã‚¢: {model.best_score_['valid_0']}")
```

---

### éšå±¤4: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ã®å†èª¿æ•´ âš–ï¸

#### 4.1 ç¾åœ¨ã®é‡ã¿ï¼ˆå•é¡Œã‚ã‚Šï¼‰

```python
# ç¾åœ¨ã®é‡ã¿
weight_binary = 0.3      # 30%
weight_ranking = 0.5     # 50% â† é«˜ã™ãã‚‹
weight_regression = 0.2  # 20%
```

**å•é¡Œç‚¹:**
- **Ranking ã®é‡ã¿ãŒ 50%** â†’ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®èª¤å·®ãŒã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å…¨ä½“ã«å¤§ããå½±éŸ¿
- Ranking ãƒ¢ãƒ‡ãƒ«ã¯å¹³å‡ã‚¹ã‚³ã‚¢ -0.5090 â†’ è² ã®å€¤ã§ä¸å®‰å®š
- Regression ãƒ¢ãƒ‡ãƒ«ãŒè»½è¦–ã•ã‚Œã¦ã„ã‚‹ï¼ˆ20%ï¼‰

#### 4.2 æ”¹å–„æ¡ˆ1: ãƒãƒ©ãƒ³ã‚¹å‹é‡ã¿

```python
# ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆå‡ç­‰é‡ã¿ï¼‰
weight_binary = 0.4      # 40% (10%å¢—)
weight_ranking = 0.3     # 30% (20%æ¸›)
weight_regression = 0.3  # 30% (10%å¢—)
```

**æ ¹æ‹ :**
- Binary: å‡ºèµ°åˆ¤å®šã¯åŸºæœ¬ã‹ã¤é‡è¦ â†’ 40%
- Ranking: ä¸å®‰å®šãªãŸã‚æ§ãˆã‚ã« â†’ 30%
- Regression: ã‚¿ã‚¤ãƒ äºˆæ¸¬ã¯å®‰å®šã—ã¦ã„ã‚‹ â†’ 30%

#### 4.3 æ”¹å–„æ¡ˆ2: ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³é‡ã¿ï¼ˆæ¨å¥¨ï¼‰

```python
def calculate_optimal_weights(binary_csv, ranking_csv, regression_csv, actual_results_csv):
    """
    éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€é©ãªé‡ã¿ã‚’è¨ˆç®—
    """
    # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’èª­ã¿è¾¼ã¿
    df_binary = pd.read_csv(binary_csv)
    df_ranking = pd.read_csv(ranking_csv)
    df_regression = pd.read_csv(regression_csv)
    df_actual = pd.read_csv(actual_results_csv)
    
    # æ­£è¦åŒ–ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    df_binary['score_norm'] = normalize_score(df_binary['binary_probability'], ascending=False)
    df_ranking['score_norm'] = normalize_score(df_ranking['ranking_score'], ascending=False)
    df_regression['score_norm'] = normalize_score(df_regression['predicted_time'], ascending=True)
    
    # å®Ÿéš›ã®ç€é †ã¨ç›¸é–¢ã‚’è¨ˆç®—
    corr_binary = df_binary['score_norm'].corr(df_actual['chakujun'])
    corr_ranking = df_ranking['score_norm'].corr(df_actual['chakujun'])
    corr_regression = df_regression['score_norm'].corr(df_actual['chakujun'])
    
    # ç›¸é–¢ã®é€†æ•°ã‚’é‡ã¿ã¨ã™ã‚‹ï¼ˆç›¸é–¢ãŒé«˜ã„ã»ã©é‡ã¿ãŒå¤§ãã„ï¼‰
    weight_binary = abs(corr_binary)
    weight_ranking = abs(corr_ranking)
    weight_regression = abs(corr_regression)
    
    # æ­£è¦åŒ–ï¼ˆåˆè¨ˆã‚’1.0ã«ã™ã‚‹ï¼‰
    total = weight_binary + weight_ranking + weight_regression
    weight_binary /= total
    weight_ranking /= total
    weight_regression /= total
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³æœ€é©é‡ã¿:")
    print(f"  - Binary: {weight_binary:.1%} (ç›¸é–¢: {corr_binary:.3f})")
    print(f"  - Ranking: {weight_ranking:.1%} (ç›¸é–¢: {corr_ranking:.3f})")
    print(f"  - Regression: {weight_regression:.1%} (ç›¸é–¢: {corr_regression:.3f})")
    
    return weight_binary, weight_ranking, weight_regression
```

#### 4.4 æ”¹å–„æ¡ˆ3: ãƒ¬ãƒ¼ã‚¹ç¨®åˆ¥ã”ã¨ã®å‹•çš„é‡ã¿

```python
def dynamic_ensemble_weights(race_info):
    """
    ãƒ¬ãƒ¼ã‚¹ã®ç‰¹æ€§ã«å¿œã˜ã¦é‡ã¿ã‚’å‹•çš„ã«å¤‰æ›´
    """
    weights = {
        'binary': 0.4,
        'ranking': 0.3,
        'regression': 0.3
    }
    
    # çŸ­è·é›¢ãƒ¬ãƒ¼ã‚¹ï¼ˆ1200mä»¥ä¸‹ï¼‰: ã‚¿ã‚¤ãƒ é‡è¦–
    if race_info['kyori'] <= 1200:
        weights['regression'] = 0.4
        weights['ranking'] = 0.3
        weights['binary'] = 0.3
    
    # é•·è·é›¢ãƒ¬ãƒ¼ã‚¹ï¼ˆ2000mä»¥ä¸Šï¼‰: ã‚¹ã‚¿ãƒŸãƒŠï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰é‡è¦–
    elif race_info['kyori'] >= 2000:
        weights['ranking'] = 0.4
        weights['regression'] = 0.3
        weights['binary'] = 0.3
    
    # å¤šé ­æ•°ãƒ¬ãƒ¼ã‚¹ï¼ˆ12é ­ä»¥ä¸Šï¼‰: å‡ºèµ°ç¢ºç‡é‡è¦–
    elif race_info['shusso_tosu'] >= 12:
        weights['binary'] = 0.5
        weights['ranking'] = 0.3
        weights['regression'] = 0.2
    
    return weights
```

---

## ğŸ“‹ å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### ãƒ•ã‚§ãƒ¼ã‚º1: ç·Šæ€¥å¯¾å¿œï¼ˆå³æ—¥ã€œ2æ—¥ï¼‰ğŸš¨

#### Step 1.1: ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã®å®Œå…¨ä¿®æ­£

```batch
# ãƒ•ã‚¡ã‚¤ãƒ«å: run_all_optimized_RECOVERY.bat
# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: UTF-8 (BOMç„¡ã—)
# æ”¹è¡Œã‚³ãƒ¼ãƒ‰: CRLF

@echo off
REM ============================================================
REM  åœ°æ–¹ç«¶é¦¬AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  Phase 7-8-5 ç²¾åº¦å›å¾©ç‰ˆ
REM  Encoding: UTF-8 without BOM
REM  Line Ending: CRLF
REM ============================================================

REM Bootstrap: Reentry architecture
chcp 65001 > nul
if "%~1"=="__REENTRY__" goto :MAIN_LOGIC
cmd /c "%~f0" __REENTRY__ %*
exit /b

:MAIN_LOGIC
shift /1
setlocal enabledelayedexpansion

REM Environment setup
set PYTHONUTF8=1
set PYTHONIOENCODING=utf-8

REM Arguments
set KEIBAJO_CODE=%~1
set TARGET_DATE=%~2

if "%KEIBAJO_CODE%"=="" (
    echo Usage: run_all_optimized_RECOVERY.bat [venue_code] [date]
    echo Example: run_all_optimized_RECOVERY.bat 43 2026-02-13
    exit /b 1
)

if "%TARGET_DATE%"=="" (
    echo ERROR: Target date is required
    exit /b 1
)

REM Venue name mapping
set "KEIBAJO_NAME="
if "%KEIBAJO_CODE%"=="30" set "KEIBAJO_NAME=é–€åˆ¥"
if "%KEIBAJO_CODE%"=="35" set "KEIBAJO_NAME=ç››å²¡"
if "%KEIBAJO_CODE%"=="36" set "KEIBAJO_NAME=æ°´æ²¢"
if "%KEIBAJO_CODE%"=="42" set "KEIBAJO_NAME=æµ¦å’Œ"
if "%KEIBAJO_CODE%"=="43" set "KEIBAJO_NAME=èˆ¹æ©‹"
if "%KEIBAJO_CODE%"=="44" set "KEIBAJO_NAME=å¤§äº•"
if "%KEIBAJO_CODE%"=="45" set "KEIBAJO_NAME=å·å´"
if "%KEIBAJO_CODE%"=="46" set "KEIBAJO_NAME=é‡‘æ²¢"
if "%KEIBAJO_CODE%"=="47" set "KEIBAJO_NAME=ç¬ æ¾"
if "%KEIBAJO_CODE%"=="48" set "KEIBAJO_NAME=åå¤å±‹"
if "%KEIBAJO_CODE%"=="50" set "KEIBAJO_NAME=åœ’ç”°"
if "%KEIBAJO_CODE%"=="51" set "KEIBAJO_NAME=å§«è·¯"
if "%KEIBAJO_CODE%"=="54" set "KEIBAJO_NAME=é«˜çŸ¥"
if "%KEIBAJO_CODE%"=="55" set "KEIBAJO_NAME=ä½è³€"

if "!KEIBAJO_NAME!"=="" (
    echo ERROR: Invalid venue code: %KEIBAJO_CODE%
    exit /b 1
)

echo ============================================================
echo   åœ°æ–¹ç«¶é¦¬AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç²¾åº¦å›å¾©ç‰ˆï¼‰
echo ============================================================
echo ç«¶é¦¬å ´: !KEIBAJO_NAME! (Code: %KEIBAJO_CODE%)
echo å®Ÿè¡Œæ—¥: %TARGET_DATE%
echo ============================================================

REM Phase 0: Data acquisition
echo.
echo [Phase 0] ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹...
python scripts\phase0_data_acquisition\extract_race_data.py %KEIBAJO_CODE% %TARGET_DATE%
if errorlevel 1 (
    echo [ERROR] Phase 0 failed
    exit /b 1
)
echo [Phase 0] å®Œäº† âœ“

REM Phase 1: Feature engineering
echo.
echo [Phase 1] ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...
python scripts\phase1_feature_engineering\feature_engineering.py %KEIBAJO_CODE% %TARGET_DATE%
if errorlevel 1 (
    echo [ERROR] Phase 1 failed
    exit /b 1
)
echo [Phase 1] å®Œäº† âœ“

REM Phase 7: Binary prediction (improved feature selection)
echo.
echo [Phase 7] äºŒå€¤åˆ†é¡äºˆæ¸¬é–‹å§‹ï¼ˆæ”¹å–„ç‰ˆç‰¹å¾´é‡é¸æŠï¼‰...
python scripts\phase7_binary\predict_optimized_binary_RECOVERY.py %KEIBAJO_CODE% %TARGET_DATE%
if errorlevel 1 (
    echo [ERROR] Phase 7 failed
    exit /b 1
)
echo [Phase 7] å®Œäº† âœ“

REM Phase 8: Ranking prediction
echo.
echo [Phase 8-Ranking] ãƒ©ãƒ³ã‚­ãƒ³ã‚°äºˆæ¸¬é–‹å§‹...
python scripts\phase8_ranking\predict_optimized_ranking_RECOVERY.py %KEIBAJO_CODE% %TARGET_DATE%
if errorlevel 1 (
    echo [ERROR] Phase 8-Ranking failed
    exit /b 1
)
echo [Phase 8-Ranking] å®Œäº† âœ“

REM Phase 8: Regression prediction
echo.
echo [Phase 8-Regression] å›å¸°äºˆæ¸¬é–‹å§‹...
python scripts\phase8_regression\predict_optimized_regression_RECOVERY.py %KEIBAJO_CODE% %TARGET_DATE%
if errorlevel 1 (
    echo [ERROR] Phase 8-Regression failed
    exit /b 1
)
echo [Phase 8-Regression] å®Œäº† âœ“

REM Phase 5: Ensemble integration (improved weights)
echo.
echo [Phase 5] ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆé–‹å§‹ï¼ˆæ”¹å–„ç‰ˆé‡ã¿ï¼‰...
python scripts\phase5_ensemble\ensemble_optimized_RECOVERY.py ^
  "data\predictions\phase7_binary\!KEIBAJO_NAME!_%TARGET_DATE:~0,4%%TARGET_DATE:~5,2%%TARGET_DATE:~8,2%_phase7_binary.csv" ^
  "data\predictions\phase8_ranking\!KEIBAJO_NAME!_%TARGET_DATE:~0,4%%TARGET_DATE:~5,2%%TARGET_DATE:~8,2%_phase8_ranking.csv" ^
  "data\predictions\phase8_regression\!KEIBAJO_NAME!_%TARGET_DATE:~0,4%%TARGET_DATE:~5,2%%TARGET_DATE:~8,2%_phase8_regression.csv" ^
  "data\predictions\phase5\!KEIBAJO_NAME!_%TARGET_DATE:~0,4%%TARGET_DATE:~5,2%%TARGET_DATE:~8,2%_ensemble_optimized_recovery.csv" ^
  --weights 0.4 0.3 0.3
if errorlevel 1 (
    echo [ERROR] Phase 5 failed
    exit /b 1
)
echo [Phase 5] å®Œäº† âœ“

REM Phase 6: Text generation
echo.
echo [Phase 6] é…ä¿¡ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹...
call scripts\phase6_betting\DAILY_OPERATION.bat %KEIBAJO_CODE% %TARGET_DATE% "data\predictions\phase5\!KEIBAJO_NAME!_%TARGET_DATE:~0,4%%TARGET_DATE:~5,2%%TARGET_DATE:~8,2%_ensemble_optimized_recovery.csv"
if errorlevel 1 (
    echo [WARNING] Phase 6 failed (text generation)
)

echo.
echo ============================================================
echo   å…¨ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†
echo ============================================================
echo äºˆæ¸¬çµæœ: data\predictions\phase5\!KEIBAJO_NAME!_%TARGET_DATE:~0,4%%TARGET_DATE:~5,2%%TARGET_DATE:~8,2%_ensemble_optimized_recovery.csv
echo ============================================================

endlocal
```

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ:**
- [ ] VS Code ã§é–‹ã
- [ ] ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: UTF-8 (BOMç„¡ã—) ã‚’ç¢ºèª
- [ ] æ”¹è¡Œã‚³ãƒ¼ãƒ‰: CRLF ã‚’ç¢ºèª
- [ ] ä¿å­˜
- [ ] `E:\anonymous-keiba-ai\run_all_optimized_RECOVERY.bat` ã«é…ç½®

#### Step 1.2: äºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ”¹å–„ç‰ˆä½œæˆ

**Binary äºˆæ¸¬ï¼ˆç‰¹å¾´é‡é¸æŠæ”¹å–„ç‰ˆï¼‰:**

ãƒ•ã‚¡ã‚¤ãƒ«: `scripts/phase7_binary/predict_optimized_binary_RECOVERY.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 7 Binary Prediction (Recovery Version)
- Improved feature selection (Lasso + MI hybrid)
- Minimum 30 features guarantee
- Stable feature selection
"""

import sys
import os
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.linear_model import LassoCV
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import StandardScaler
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# æœ€ä½ç‰¹å¾´é‡æ•°
MIN_FEATURES_BINARY = 30

def lasso_feature_selection(X, y, min_features=MIN_FEATURES_BINARY):
    """Lassoæ­£å‰‡åŒ–ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ"""
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    lasso = LassoCV(
        alphas=np.logspace(-4, 1, 100),
        cv=5,
        max_iter=10000,
        random_state=42
    )
    lasso.fit(X_scaled, y)
    
    selected_mask = np.abs(lasso.coef_) > 1e-5
    selected_features = X.columns[selected_mask].tolist()
    
    if len(selected_features) < min_features:
        top_indices = np.argsort(np.abs(lasso.coef_))[::-1][:min_features]
        selected_features = X.columns[top_indices].tolist()
    
    return selected_features

def mutual_info_selection(X, y, top_k=35):
    """ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ"""
    mi_scores = mutual_info_classif(
        X, y,
        discrete_features='auto',
        random_state=42,
        n_neighbors=3
    )
    
    top_indices = np.argsort(mi_scores)[::-1][:top_k]
    selected_features = X.columns[top_indices].tolist()
    
    return selected_features

def hybrid_feature_selection(X, y):
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰¹å¾´é‡é¸æŠ
    - Lasso ã¨ ç›¸äº’æƒ…å ±é‡ã®ä¸¡æ–¹ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’å„ªå…ˆ
    - æœ€ä½30ç‰¹å¾´é‡ã‚’ä¿è¨¼
    """
    print(f"\n[ç‰¹å¾´é‡é¸æŠ] ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•")
    
    # Lasso é¸æŠ
    lasso_features = lasso_feature_selection(X, y, min_features=MIN_FEATURES_BINARY)
    print(f"  - Lasso: {len(lasso_features)}å€‹")
    
    # ç›¸äº’æƒ…å ±é‡é¸æŠ
    mi_features = mutual_info_selection(X, y, top_k=35)
    print(f"  - ç›¸äº’æƒ…å ±é‡: {len(mi_features)}å€‹")
    
    # å¤šæ•°æ±º: ä¸¡æ–¹ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡
    feature_votes = defaultdict(int)
    for feat in lasso_features:
        feature_votes[feat] += 1
    for feat in mi_features:
        feature_votes[feat] += 1
    
    # 2ã¤ã®æ‰‹æ³•ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡
    consensus_features = [
        feat for feat, count in feature_votes.items()
        if count >= 2
    ]
    
    # æœ€ä½ç‰¹å¾´é‡æ•°ã‚’ç¢ºä¿
    if len(consensus_features) < MIN_FEATURES_BINARY:
        print(f"  âš ï¸  ç‰¹å¾´é‡æ•°ä¸è¶³: {len(consensus_features)} < {MIN_FEATURES_BINARY}")
        print(f"  â†’ Lasso ãƒˆãƒƒãƒ—{MIN_FEATURES_BINARY}å€‹ã‚’ä½¿ç”¨")
        consensus_features = lasso_features[:MIN_FEATURES_BINARY]
    
    print(f"  âœ… æœ€çµ‚é¸æŠ: {len(consensus_features)}å€‹")
    
    return consensus_features

def predict_binary(venue_code, target_date):
    """Binary prediction with improved feature selection"""
    print(f"\n{'='*80}")
    print(f"Phase 7 Binary Prediction (Recovery Version)")
    print(f"{'='*80}")
    
    # Venue name mapping
    venue_names = {
        30: "monbetsu", 35: "morioka", 36: "mizusawa",
        42: "urawa", 43: "funabashi", 44: "ooi",
        45: "kawasaki", 46: "kanazawa", 47: "kasamatsu",
        48: "nagoya", 50: "sonoda", 51: "himeji",
        54: "kochi", 55: "saga"
    }
    
    venue_romaji = venue_names.get(int(venue_code))
    if not venue_romaji:
        print(f"âŒ Invalid venue code: {venue_code}")
        return None
    
    # Load model
    model_path = f"data/models/tuned/{venue_romaji}_tuned_model.txt"
    if not os.path.exists(model_path):
        print(f"âŒ Model not found: {model_path}")
        return None
    
    model = lgb.Booster(model_file=model_path)
    print(f"âœ… Model loaded: {model_path}")
    print(f"  - Features: {model.num_feature()}")
    
    # Load prediction data
    date_str = target_date.replace('-', '')
    input_csv = f"data/featured/2026/02/èˆ¹æ©‹_{date_str}_featured.csv"  # Need venue name
    
    if not os.path.exists(input_csv):
        print(f"âŒ Input file not found: {input_csv}")
        return None
    
    try:
        df = pd.read_csv(input_csv, encoding='shift-jis')
    except:
        df = pd.read_csv(input_csv, encoding='utf-8')
    
    print(f"âœ… Data loaded: {len(df)} records")
    
    # Feature columns (exclude ID columns)
    id_cols = ['race_id', 'umaban', 'kaisai_nen', 'kaisai_tsukihi', 
               'keibajo_code', 'race_bango', 'ketto_toroku_bango']
    feature_cols = [col for col in df.columns if col not in id_cols]
    
    X = df[feature_cols].copy()
    
    # Fill missing values
    X = X.fillna(X.median())
    
    # Hybrid feature selection
    # Note: For prediction, use the same features as training
    # This is a simplified version - in production, load feature list from model
    feature_names = model.feature_name()
    
    # Ensure all model features exist
    missing_features = [f for f in feature_names if f not in X.columns]
    if missing_features:
        print(f"âš ï¸  Missing features: {len(missing_features)}")
        for feat in missing_features:
            X[feat] = 0
    
    # Select only model features
    X = X[feature_names]
    
    print(f"âœ… Feature selection: {len(feature_names)} features")
    
    # Predict
    y_pred = model.predict(X)
    
    # Create output dataframe
    result_df = df[id_cols].copy()
    result_df['binary_probability'] = y_pred
    result_df['binary_prediction'] = (y_pred > 0.5).astype(int)
    
    # Statistics
    print(f"\nâœ… Prediction results:")
    print(f"  - Average probability: {y_pred.mean():.4f}")
    print(f"  - Max probability: {y_pred.max():.4f}")
    print(f"  - Min probability: {y_pred.min():.4f}")
    print(f"  - Predicted in-race: {result_df['binary_prediction'].sum()} / {len(result_df)}")
    
    # Save
    output_dir = "data/predictions/phase7_binary"
    os.makedirs(output_dir, exist_ok=True)
    output_path = f"{output_dir}/èˆ¹æ©‹_{date_str}_phase7_binary_recovery.csv"
    
    try:
        result_df.to_csv(output_path, index=False, encoding='shift-jis')
    except:
        result_df.to_csv(output_path, index=False, encoding='utf-8')
    
    print(f"\nâœ… Results saved: {output_path}")
    
    return result_df

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python predict_optimized_binary_RECOVERY.py <venue_code> <target_date>")
        print("Example: python predict_optimized_binary_RECOVERY.py 43 2026-02-13")
        sys.exit(1)
    
    venue_code = sys.argv[1]
    target_date = sys.argv[2]
    
    try:
        result = predict_binary(venue_code, target_date)
        if result is not None:
            print("\n" + "="*80)
            print("âœ… Phase 7 Binary Prediction Completed (Recovery Version)")
            print("="*80)
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
```

**åŒæ§˜ã« Ranking ã¨ Regression ã® RECOVERY ç‰ˆã‚‚ä½œæˆã—ã¾ã™ï¼ˆçœç•¥ï¼‰ã€‚**

#### Step 1.3: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿èª¿æ•´ç‰ˆ

ãƒ•ã‚¡ã‚¤ãƒ«: `scripts/phase5_ensemble/ensemble_optimized_RECOVERY.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 5 Ensemble (Recovery Version)
- Improved weights: Binary=0.4, Ranking=0.3, Regression=0.3
- Better balance to reduce Ranking dominance
"""

# ... (å‰åŠã¯æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨åŒã˜)

if __name__ == "__main__":
    # ... (å¼•æ•°å‡¦ç†)
    
    # æ”¹å–„ç‰ˆã®é‡ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    weight_binary = 0.4      # 0.3 â†’ 0.4 (10%å¢—)
    weight_ranking = 0.3     # 0.5 â†’ 0.3 (20%æ¸›)
    weight_regression = 0.3  # 0.2 â†’ 0.3 (10%å¢—)
    
    # ... (ä»¥ä¸‹åŒã˜)
```

---

### ãƒ•ã‚§ãƒ¼ã‚º2: ä¸­æœŸæ”¹å–„ï¼ˆ3-7æ—¥ï¼‰ğŸ“Š

#### Step 2.1: ç‰¹å¾´é‡é¸æŠã®å…¨ç«¶é¦¬å ´å†å®Ÿè¡Œ

```python
# ãƒ•ã‚¡ã‚¤ãƒ«: retrain_all_venues_recovery.py

venues = [30, 35, 36, 42, 43, 44, 45, 46, 47, 48, 50, 51, 54, 55]

for venue_code in venues:
    print(f"\n{'='*80}")
    print(f"[{venue_code}] ç‰¹å¾´é‡é¸æŠ & å†å­¦ç¿’é–‹å§‹...")
    print(f"{'='*80}")
    
    # Phase 7: Binary (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰¹å¾´é‡é¸æŠ)
    os.system(f"python scripts/phase7_feature_selection/select_features_hybrid.py {venue_code} binary")
    os.system(f"python scripts/phase7_binary/train_binary_model.py {venue_code}")
    
    # Phase 8: Ranking
    os.system(f"python scripts/phase7_feature_selection/select_features_hybrid.py {venue_code} ranking")
    os.system(f"python scripts/phase8_ranking/train_ranking_model.py {venue_code}")
    
    # Phase 8: Regression
    os.system(f"python scripts/phase7_feature_selection/select_features_hybrid.py {venue_code} regression")
    os.system(f"python scripts/phase8_regression/train_regression_model.py {venue_code}")
    
    print(f"\nâœ… [{venue_code}] å®Œäº†")
```

**å®Ÿè¡Œ:**
```cmd
cd E:\anonymous-keiba-ai
python retrain_all_venues_recovery.py
```

#### Step 2.2: Optuna å†æœ€é©åŒ–

```python
# ãƒ•ã‚¡ã‚¤ãƒ«: reoptimize_hyperparameters_recovery.py

import optuna
from sklearn.model_selection import TimeSeriesSplit

def objective_recovery(trial, X, y):
    """æ”¹å–„ç‰ˆç›®çš„é–¢æ•°"""
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'max_depth': trial.suggest_int('max_depth', 4, 8),
        'num_leaves': trial.suggest_int('num_leaves', 31, 100),
        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100)
    }
    
    model = lgb.LGBMClassifier(**params, random_state=42)
    
    # æ™‚ç³»åˆ—åˆ†å‰²ã§è©•ä¾¡
    tscv = TimeSeriesSplit(n_splits=5)
    
    recalls = []
    for train_idx, val_idx in tscv.split(X):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
        
        model.fit(X_train_fold, y_train_fold)
        y_pred = (model.predict_proba(X_val_fold)[:, 1] > 0.5).astype(int)
        
        recall = recall_score(y_val_fold, y_pred)
        recalls.append(recall)
    
    # Recall ã®å¹³å‡ã‚’æœ€å¤§åŒ–ï¼ˆç¬¦å·åè»¢ï¼‰
    return -np.mean(recalls)

# å…¨ç«¶é¦¬å ´ã§å®Ÿè¡Œ
for venue_code in venues:
    study = optuna.create_study(direction='minimize')
    study.optimize(lambda trial: objective_recovery(trial, X, y), n_trials=100)
    
    best_params = study.best_params
    print(f"[{venue_code}] Best params: {best_params}")
    
    # ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’
    # ...
```

#### Step 2.3: ç²¾åº¦æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ

```python
# ãƒ•ã‚¡ã‚¤ãƒ«: compare_old_vs_recovery.py

def compare_models(venue_code, target_date):
    """æ—§ãƒ¢ãƒ‡ãƒ« vs å›å¾©ç‰ˆãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ"""
    
    # æ—§ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
    old_predictions = pd.read_csv(f"data/predictions/phase5/èˆ¹æ©‹_{target_date}_ensemble_optimized.csv")
    
    # å›å¾©ç‰ˆãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
    recovery_predictions = pd.read_csv(f"data/predictions/phase5/èˆ¹æ©‹_{target_date}_ensemble_optimized_recovery.csv")
    
    # å®Ÿéš›ã®çµæœ
    actual_results = pd.read_csv(f"data/results/èˆ¹æ©‹_{target_date}_results.csv")
    
    # Top 3 ã®çš„ä¸­ç‡ã‚’è¨ˆç®—
    def calculate_hit_rate(predictions, actual, top_n=3):
        hits = 0
        total_races = predictions['race_id'].nunique()
        
        for race_id in predictions['race_id'].unique():
            race_pred = predictions[predictions['race_id'] == race_id].nlargest(top_n, 'ensemble_score')
            race_actual = actual[actual['race_id'] == race_id].nsmallest(3, 'chakujun')
            
            # äºˆæ¸¬ãƒˆãƒƒãƒ—3ã¨å®Ÿéš›ã®ä¸Šä½3ãŒé‡ãªã£ã¦ã„ã‚‹ã‹
            pred_horses = set(race_pred['umaban'])
            actual_horses = set(race_actual['umaban'])
            
            if len(pred_horses & actual_horses) > 0:
                hits += 1
        
        return hits / total_races
    
    old_hit_rate = calculate_hit_rate(old_predictions, actual_results)
    recovery_hit_rate = calculate_hit_rate(recovery_predictions, actual_results)
    
    print(f"{'='*80}")
    print(f"ç²¾åº¦æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ")
    print(f"{'='*80}")
    print(f"æ—§ãƒ¢ãƒ‡ãƒ« çš„ä¸­ç‡: {old_hit_rate:.1%}")
    print(f"å›å¾©ç‰ˆãƒ¢ãƒ‡ãƒ« çš„ä¸­ç‡: {recovery_hit_rate:.1%}")
    print(f"æ”¹å–„åº¦: {(recovery_hit_rate - old_hit_rate):.1%}")
    print(f"{'='*80}")
```

---

### ãƒ•ã‚§ãƒ¼ã‚º3: é•·æœŸå®‰å®šåŒ–ï¼ˆ1-2é€±é–“ï¼‰ğŸ”’

#### Step 3.1: PowerShell ç§»è¡Œ

```powershell
# ãƒ•ã‚¡ã‚¤ãƒ«: run_all_optimized_RECOVERY.ps1

param(
    [Parameter(Mandatory=$true)]
    [int]$VenueCode,
    
    [Parameter(Mandatory=$true)]
    [string]$TargetDate
)

# UTF-8 è¨­å®š
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8
$env:PYTHONUTF8 = "1"
$env:PYTHONIOENCODING = "utf-8"

# ç«¶é¦¬å ´åãƒãƒƒãƒ”ãƒ³ã‚°
$VenueNames = @{
    43 = "èˆ¹æ©‹"
    # ... (çœç•¥)
}

$VenueName = $VenueNames[$VenueCode]

Write-Host "==========================================" -ForegroundColor Cyan
Write-Host "  åœ°æ–¹ç«¶é¦¬AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå›å¾©ç‰ˆï¼‰" -ForegroundColor Cyan
Write-Host "==========================================" -ForegroundColor Cyan
Write-Host "ç«¶é¦¬å ´: $VenueName" -ForegroundColor Yellow
Write-Host "å®Ÿè¡Œæ—¥: $TargetDate" -ForegroundColor Yellow

# Phase 0-8 å®Ÿè¡Œ
# ... (çœç•¥)
```

#### Step 3.2: è‡ªå‹•ãƒ†ã‚¹ãƒˆ & CI/CD

```yaml
# ãƒ•ã‚¡ã‚¤ãƒ«: .github/workflows/model_accuracy_check.yml

name: Model Accuracy Check

on:
  push:
    branches: [ main, phase0_complete_fix_* ]
  pull_request:
    branches: [ main ]

jobs:
  accuracy-test:
    runs-on: windows-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run accuracy tests
      run: |
        python tests/test_model_accuracy.py
    
    - name: Check hit rate threshold
      run: |
        python tests/check_hit_rate_threshold.py --min-hit-rate 0.70
```

---

## ğŸ“Š æˆåŠŸåŸºæº–ã¨æ¤œè¨¼æ–¹æ³•

### å³æ™‚ç›®æ¨™ï¼ˆãƒ•ã‚§ãƒ¼ã‚º1å®Œäº†æ™‚ï¼‰

- [ ] ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ãŒ UTF-8 BOM ã‚¨ãƒ©ãƒ¼ãªãå®Ÿè¡Œã§ãã‚‹
- [ ] Phase 7-8 ã®ç‰¹å¾´é‡æ•°ãŒ 30å€‹ä»¥ä¸Š
- [ ] ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ãŒ Binary=0.4, Ranking=0.3, Regression=0.3
- [ ] äºˆæ¸¬CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã‚‹

### ä¸­æœŸç›®æ¨™ï¼ˆãƒ•ã‚§ãƒ¼ã‚º2å®Œäº†æ™‚ï¼‰

- [ ] 1ãƒ»2ä½é¦¬ã®è¤‡å‹çš„ä¸­ç‡ãŒ **70%ä»¥ä¸Š** ã«å›å¾©
- [ ] å…¨14ç«¶é¦¬å ´ã§å®‰å®šå‹•ä½œ
- [ ] æ—§ãƒ¢ãƒ‡ãƒ«ã¨ã®ç²¾åº¦æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆãŒä½œæˆã•ã‚Œã¦ã„ã‚‹

### é•·æœŸç›®æ¨™ï¼ˆãƒ•ã‚§ãƒ¼ã‚º3å®Œäº†æ™‚ï¼‰

- [ ] 1ãƒ»2ä½é¦¬ã®è¤‡å‹çš„ä¸­ç‡ãŒ **80%ä»¥ä¸Š** ã‚’é”æˆ
- [ ] PowerShell ã¸ã®ç§»è¡ŒãŒå®Œäº†
- [ ] è‡ªå‹•ãƒ†ã‚¹ãƒˆãŒ CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹
- [ ] é€±æ¬¡ãƒ»æœˆæ¬¡ãƒ¬ãƒãƒ¼ãƒˆãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹

---

## ğŸ” æ¤œè¨¼æ‰‹é †

### æ‰‹é †1: å³æ™‚ãƒ†ã‚¹ãƒˆï¼ˆèˆ¹æ©‹ç«¶é¦¬å ´ï¼‰

```cmd
cd E:\anonymous-keiba-ai

REM æ—§ãƒãƒƒãƒã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
ren run_all_optimized.bat run_all_optimized.bat.old

REM å›å¾©ç‰ˆãƒãƒƒãƒã‚’ã‚³ãƒ”ãƒ¼
copy run_all_optimized_RECOVERY.bat run_all_optimized.bat

REM ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
run_all_optimized.bat 43 2026-02-13

REM çµæœç¢ºèª
dir data\predictions\phase5\èˆ¹æ©‹_20260213_ensemble_optimized_recovery.csv
type data\predictions\phase5\èˆ¹æ©‹_20260213_ensemble_optimized_recovery.csv | more
```

**æˆåŠŸæ¡ä»¶:**
- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„
- Phase 7-8-5 ãŒå…¨ã¦å®Œäº†
- CSV ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã‚‹
- æ—¥æœ¬èªãŒæ­£ã—ãè¡¨ç¤ºã•ã‚Œã‚‹

### æ‰‹é †2: ç²¾åº¦æ¯”è¼ƒï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼ï¼‰

```python
# compare_accuracy.py

import pandas as pd

# æ—§ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ï¼ˆ2026-02-07ãªã©ï¼‰
old_csv = "data/predictions/phase5/èˆ¹æ©‹_20260207_ensemble_optimized.csv"
old_df = pd.read_csv(old_csv, encoding='shift-jis')

# å›å¾©ç‰ˆãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ï¼ˆåŒã˜æ—¥ä»˜ã§å†å®Ÿè¡Œï¼‰
recovery_csv = "data/predictions/phase5/èˆ¹æ©‹_20260207_ensemble_optimized_recovery.csv"
recovery_df = pd.read_csv(recovery_csv, encoding='shift-jis')

# å®Ÿéš›ã®çµæœ
actual_csv = "data/results/èˆ¹æ©‹_20260207_results.csv"
actual_df = pd.read_csv(actual_csv, encoding='shift-jis')

# Top3 çš„ä¸­ç‡ã‚’è¨ˆç®—
def calculate_top3_hit_rate(pred_df, actual_df):
    hits = 0
    total = pred_df['race_id'].nunique()
    
    for race_id in pred_df['race_id'].unique():
        race_pred = pred_df[pred_df['race_id'] == race_id].nlargest(3, 'ensemble_score')
        race_actual = actual_df[actual_df['race_id'] == race_id].nsmallest(3, 'chakujun')
        
        pred_horses = set(race_pred['umaban'])
        actual_horses = set(race_actual['umaban'])
        
        # 1é ­ã§ã‚‚å½“ãŸã‚Œã°çš„ä¸­
        if len(pred_horses & actual_horses) > 0:
            hits += 1
    
    return hits / total

old_rate = calculate_top3_hit_rate(old_df, actual_df)
recovery_rate = calculate_top3_hit_rate(recovery_df, actual_df)

print(f"æ—§ãƒ¢ãƒ‡ãƒ« Top3çš„ä¸­ç‡: {old_rate:.1%}")
print(f"å›å¾©ç‰ˆãƒ¢ãƒ‡ãƒ« Top3çš„ä¸­ç‡: {recovery_rate:.1%}")
print(f"æ”¹å–„åº¦: {(recovery_rate - old_rate) * 100:.1f} ãƒã‚¤ãƒ³ãƒˆ")

# ç›®æ¨™é”æˆåˆ¤å®š
if recovery_rate >= 0.70:
    print("\nâœ… ç›®æ¨™é”æˆï¼ï¼ˆ70%ä»¥ä¸Šï¼‰")
else:
    print(f"\nâš ï¸  ç›®æ¨™æœªé”ï¼ˆç¾åœ¨ {recovery_rate:.1%}ã€ç›®æ¨™ 70%ï¼‰")
```

### æ‰‹é †3: 14ç«¶é¦¬å ´ã§ã®ä¸€æ‹¬æ¤œè¨¼

```cmd
cd E:\anonymous-keiba-ai

REM å…¨ç«¶é¦¬å ´ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
FOR %%V IN (30 35 36 42 43 44 45 46 47 48 50 51 54 55) DO (
    echo [TEST] ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰: %%V
    run_all_optimized.bat %%V 2026-02-14
    if errorlevel 1 (
        echo [FAIL] ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ %%V ã§å¤±æ•—
        pause
    ) else (
        echo [OK] ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ %%V æˆåŠŸ
    )
)

echo å…¨ç«¶é¦¬å ´ãƒ†ã‚¹ãƒˆå®Œäº†
pause
```

---

## ğŸ“ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ: ç‰¹å¾´é‡æ•°ãŒä¾ç„¶ã¨ã—ã¦å°‘ãªã„ï¼ˆ< 30å€‹ï¼‰

**ç—‡çŠ¶:**
```
Phase 7 Binary: 25 features selected (< 30 minimum)
```

**è§£æ±ºç­–:**
```python
# scripts/phase7_binary/predict_optimized_binary_RECOVERY.py ã®ä¿®æ­£

MIN_FEATURES_BINARY = 30  # å¿…ãš30å€‹ä»¥ä¸Šã«ã™ã‚‹

# ç‰¹å¾´é‡é¸æŠå¾Œã®ãƒã‚§ãƒƒã‚¯ã‚’å¼·åŒ–
if len(selected_features) < MIN_FEATURES_BINARY:
    print(f"âš ï¸  ç‰¹å¾´é‡æ•°ä¸è¶³: å¼·åˆ¶çš„ã« {MIN_FEATURES_BINARY} å€‹ã«å¢—ã‚„ã—ã¾ã™")
    
    # å…¨ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’è¨ˆç®—
    rf = RandomForestClassifier(n_jobs=-1, random_state=42)
    rf.fit(X, y)
    importances = rf.feature_importances_
    
    # é‡è¦åº¦ãƒˆãƒƒãƒ—30ã‚’é¸æŠ
    top_indices = np.argsort(importances)[::-1][:MIN_FEATURES_BINARY]
    selected_features = X.columns[top_indices].tolist()
    
    print(f"âœ… é‡è¦åº¦ãƒˆãƒƒãƒ— {MIN_FEATURES_BINARY} å€‹ã‚’æ¡ç”¨")
```

### å•é¡Œ: Ranking ã®é‡ã¿ãŒé«˜ã„ã¾ã¾

**ç—‡çŠ¶:**
```
Ensemble weights: Binary=0.3, Ranking=0.5, Regression=0.2
```

**è§£æ±ºç­–:**
```python
# scripts/phase5_ensemble/ensemble_optimized_RECOVERY.py ã®ç¢ºèª

# å¿…ãšä»¥ä¸‹ã®é‡ã¿ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
weight_binary = 0.4
weight_ranking = 0.3
weight_regression = 0.3

# ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®å¼•æ•°ã‚‚ç¢ºèª
# run_all_optimized.bat ã® Phase 5 å‘¼ã³å‡ºã—è¡Œ:
python scripts\phase5_ensemble\ensemble_optimized_RECOVERY.py ... --weights 0.4 0.3 0.3
```

### å•é¡Œ: ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶:**
```
'NCODING' ã¯ã€å†…éƒ¨ã‚³ãƒãƒ³ãƒ‰ã¾ãŸã¯å¤–éƒ¨ã‚³ãƒãƒ³ãƒ‰ã€
æ“ä½œå¯èƒ½ãªãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¾ãŸã¯ãƒãƒƒãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦èªè­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚
```

**è§£æ±ºç­–:**
```powershell
# PowerShell ã§ BOM ã‚’ãƒã‚§ãƒƒã‚¯
Get-Content run_all_optimized_RECOVERY.bat -Encoding Byte | Select-Object -First 3

# æœŸå¾…å€¤: 40 65 63 (@ e c ã® ASCII)
# NGå€¤: EF BB BF (UTF-8 BOM)

# BOM ãŒã‚ã£ãŸå ´åˆã€VS Code ã§å†ä¿å­˜:
# 1. VS Code ã§é–‹ã
# 2. å³ä¸‹ã® "UTF-8 with BOM" ã‚’ã‚¯ãƒªãƒƒã‚¯
# 3. "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä»˜ãã§ä¿å­˜" â†’ "UTF-8" ã‚’é¸æŠï¼ˆBOMç„¡ã—ï¼‰
# 4. ä¿å­˜
```

---

## ğŸ¯ æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

### å®šé‡çš„åŠ¹æœ

| æŒ‡æ¨™ | æ—§ãƒ¢ãƒ‡ãƒ« | æ–°ãƒ¢ãƒ‡ãƒ«ï¼ˆæ”¹å–„å‰ï¼‰ | å›å¾©ç‰ˆï¼ˆç›®æ¨™ï¼‰ |
|------|----------|-------------------|---------------|
| **1ãƒ»2ä½è¤‡å‹çš„ä¸­ç‡** | 89% | 50%æœªæº€ | **80%ä»¥ä¸Š** |
| **Top3çš„ä¸­ç‡** | 75% | 40% | **70%ä»¥ä¸Š** |
| **Recall** | 0.35 | 0.012 | **0.30ä»¥ä¸Š** |
| **Return Rate** | 1.15 | 0.85 | **1.10ä»¥ä¸Š** |
| **ç‰¹å¾´é‡æ•°ï¼ˆBinaryï¼‰** | 40-50 | 24-31 | **30-40** |
| **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®‰å®šæ€§** | ä¸­ | ä½ | **é«˜** |

### å®šæ€§çš„åŠ¹æœ

1. **äºˆæ¸¬ã®å®‰å®šæ€§å‘ä¸Š**
   - æ¥µç«¯ãªäºˆæ¸¬ãŒæ¸›å°‘
   - ãƒ¬ãƒ¼ã‚¹é–“ã§ã®äºˆæ¸¬ãƒãƒ©ã¤ããŒæ”¹å–„

2. **å®Ÿç”¨æ€§ã®å›å¾©**
   - å®Ÿéš›ã«è³­ã‘ã¦åˆ©ç›ŠãŒå‡ºã‚‹ãƒ¬ãƒ™ãƒ«ã«å›å¾©
   - çš„ä¸­ç‡ãŒä¿¡é ¼ã§ãã‚‹ç¯„å›²ã«

3. **é‹ç”¨ã®ä¿¡é ¼æ€§å‘ä¸Š**
   - ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼ãŒãªããªã‚‹
   - 14ç«¶é¦¬å ´ã§å®‰å®šå‹•ä½œ

4. **ä¿å®ˆæ€§ã®å‘ä¸Š**
   - PowerShell ç§»è¡Œã§å°†æ¥çš„ãªæ‹¡å¼µãŒå®¹æ˜“ã«
   - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒæ˜ç¢ºã«

---

## ğŸ“š å‚è€ƒè³‡æ–™

### ä½œæˆæ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
1. `RECONSTRUCTION_ROADMAP.md` - Phase 7-8-5 å†æ§‹ç¯‰ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—
2. `PHASE7_14_VENUES_COMPLETE_REPORT.md` - Boruta ç‰¹å¾´é‡é¸æŠãƒ¬ãƒãƒ¼ãƒˆ
3. æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ - `ACCURACY_RECOVERY_PLAN.md`

### æŠ€è¡“å‚è€ƒè³‡æ–™
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [scikit-learn Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)
- [Time Series Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split)

### ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µãƒ¼ãƒãƒ¬ãƒãƒ¼ãƒˆ
- `åœ°æ–¹ç«¶é¦¬AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ï¼ˆPhase 7-8-5 æ–°ãƒ¢ãƒ‡ãƒ«ï¼‰ã«ãŠã‘ã‚‹äºˆæ¸¬ç²¾åº¦ä½ä¸‹ã®åŸå› ç‰¹å®šãŠã‚ˆã³å¯¾ç­–.md`
  - æ ¹æœ¬åŸå› 3ã¤ã®è©³ç´°åˆ†æ
  - å®šé‡çš„è¨¼æ‹ 
  - ã‚·ã‚¹ãƒ†ãƒ éšœå®³ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ è§£èª¬

---

**ä½œæˆæ—¥**: 2026-02-14  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0 - Recovery Plan  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: ğŸš¨ ç·Šæ€¥å®Ÿè£…ä¸­  
**å„ªå…ˆåº¦**: ğŸ”´ æœ€é«˜  
**æ‹…å½“**: anonymousç«¶é¦¬AIã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºãƒãƒ¼ãƒ   

---

## æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

### å³åº§ã«å®Ÿè¡Œã™ã¹ãã“ã¨ï¼ˆä»Šæ—¥ä¸­ï¼‰

1. **ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã®ç½®ãæ›ãˆ**
   ```cmd
   cd E:\anonymous-keiba-ai
   ren run_all_optimized.bat run_all_optimized.bat.old
   copy run_all_optimized_RECOVERY.bat run_all_optimized.bat
   ```

2. **èˆ¹æ©‹ç«¶é¦¬å ´ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**
   ```cmd
   run_all_optimized.bat 43 2026-02-13
   ```

3. **çµæœç¢ºèª**
   - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒãªã„ã‹
   - Phase 7-8 ãŒå®Ÿè¡Œã•ã‚ŒãŸã‹
   - äºˆæ¸¬CSVãŒç”Ÿæˆã•ã‚ŒãŸã‹

4. **ã‚®ãƒƒãƒˆãƒãƒ–ã«ã‚³ãƒŸãƒƒãƒˆ**
   ```bash
   git add .
   git commit -m "ğŸš¨ ç·Šæ€¥ä¿®æ­£: ç²¾åº¦å›å¾©ãƒ—ãƒ©ãƒ³å®Ÿè£… (89%â†’50%å•é¡Œå¯¾å¿œ)"
   git push
   ```

### æ˜æ—¥ä»¥é™ã®ã‚¿ã‚¹ã‚¯

- [ ] äºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆã® RECOVERY ç‰ˆã‚’å…¨ã¦ä½œæˆ
- [ ] ç‰¹å¾´é‡é¸æŠã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã‚’å®Ÿè£…
- [ ] Optuna ç›®çš„é–¢æ•°ã‚’æ”¹å–„ç‰ˆã«ç½®ãæ›ãˆ
- [ ] 14ç«¶é¦¬å ´ã§ä¸€æ‹¬ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ç²¾åº¦æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ

---

**é‡è¦**: ã“ã®è¨ˆç”»ã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µãƒ¼ãƒã«ã‚ˆã‚‹æ ¹æœ¬åŸå› åˆ†æã«åŸºã¥ãåŒ…æ‹¬çš„ãªè§£æ±ºç­–ã§ã™ã€‚
å˜ãªã‚‹ä¸€æ™‚çš„ãªå¯¾å‡¦ã§ã¯ãªãã€ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ§‹é€ çš„ãªå•é¡Œã‚’è§£æ±ºã™ã‚‹è¨­è¨ˆã«ãªã£ã¦ã„ã¾ã™ã€‚

**ç›®æ¨™**: **1ãƒ»2ä½é¦¬ã®è¤‡å‹çš„ä¸­ç‡ã‚’ 80%ä»¥ä¸Šã«å›å¾©ã•ã›ã‚‹**
